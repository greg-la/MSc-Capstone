{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33634b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "283825a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde29996",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40d78551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of columns to convert to categorical\n",
    "categorical_columns = df.select_dtypes(include='int64').columns.tolist()\n",
    "\n",
    "# Convert the selected columns to categorical\n",
    "df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "\n",
    "numeric_data = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate MAD for each column\n",
    "mad = numeric_data.mad()\n",
    "\n",
    "# Choose a threshold multiplier\n",
    "k = 3\n",
    "\n",
    "# Calculate the threshold value\n",
    "threshold = k * mad\n",
    "\n",
    "# Identify outliers\n",
    "outliers = (np.abs(numeric_data - numeric_data.median()) > threshold)\n",
    "\n",
    "# Apply logarithm to the specified columns\n",
    "outlier_columns = ['CommissionSacrificePercentage', 'BonusCommissionPercentage']\n",
    "for column in outlier_columns:\n",
    "    df[column] = np.log1p(df[column])\n",
    "    \n",
    "df = df.drop(columns=['PropDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af572aed",
   "metadata": {},
   "source": [
    "### BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921566ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in resampled dataset:\n",
      "0    97922\n",
      "1    97922\n",
      "Name: PolicyIssued, dtype: int64\n",
      "0    85682\n",
      "1    85682\n",
      "Name: PolicyIssued, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X = df.drop(\"PolicyIssued\", axis=1)\n",
    "y = df[\"PolicyIssued\"]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_70, X_test_70, y_train_70, y_test_70 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_80 = scaler.fit_transform(X_train_80)\n",
    "X_test_scaled_80 = scaler.transform(X_test_80)\n",
    "\n",
    "X_train_scaled_70 = scaler.fit_transform(X_train_70)\n",
    "X_test_scaled_70 = scaler.transform(X_test_70)\n",
    "\n",
    "# Applying BorderlineSMOTE to the training data\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "X_resampled_80, y_resampled_80 = smote.fit_resample(X_train_scaled_80, y_train_80)\n",
    "\n",
    "X_resampled_70, y_resampled_70 = smote.fit_resample(X_train_scaled_70, y_train_70)\n",
    "\n",
    "# Convert the resampled arrays back to a dataframe\n",
    "resampled_df_80 = pd.DataFrame(X_resampled_80, columns=X_train_80.columns)\n",
    "resampled_df_80[\"PolicyIssued\"] = y_resampled_80\n",
    "\n",
    "resampled_df_70 = pd.DataFrame(X_resampled_70, columns=X_train_70.columns)\n",
    "resampled_df_70[\"PolicyIssued\"] = y_resampled_70\n",
    "\n",
    "# Check the class distribution in the resampled dataset\n",
    "print(\"Class distribution in resampled dataset:\")\n",
    "print(resampled_df_80[\"PolicyIssued\"].value_counts())\n",
    "print(resampled_df_70[\"PolicyIssued\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7c410",
   "metadata": {},
   "source": [
    "# All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e5fcc",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25989a89",
   "metadata": {},
   "source": [
    "#### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7dd083d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 6088   884]\n",
      " [ 9179 15302]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55      6972\n",
      "           1       0.95      0.63      0.75     24481\n",
      "\n",
      "    accuracy                           0.68     31453\n",
      "   macro avg       0.67      0.75      0.65     31453\n",
      "weighted avg       0.82      0.68      0.71     31453\n",
      "\n",
      "\n",
      "AUC: 0.7491316400886301\n",
      "Accuracy: 0.6800623152004578\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise and train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled_80)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test_80, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_80, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "confusion_mat = confusion_matrix(y_test_80, y_pred)\n",
    "class_report = classification_report(y_test_80, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "confusion_mat = confusion_matrix(y_test_80, y_pred)\n",
    "class_report = classification_report(y_test_80, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nAUC:\", auc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b9263",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "231de654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7451126129884722\n",
      "Average Accuracy: 0.7450981408992405\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7491316400886301\n",
      "Test Accuracy: 0.6800623152004578\n",
      "---------------------------------------------\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7452153304382247\n",
      "Average Accuracy: 0.7452053720709627\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7491316400886301\n",
      "Test Accuracy: 0.6800623152004578\n",
      "---------------------------------------------\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7451369120351143\n",
      "Average Accuracy: 0.7451288328948448\n",
      "---------------------------------------------\n",
      "Test AUC: 0.7491316400886301\n",
      "Test Accuracy: 0.6800623152004578\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# List of k values for k-fold cross-validation\n",
    "num_folds_list = [5, 10, 20]\n",
    "\n",
    "# Initialise logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "    \n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test_scaled_80)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_test_80, y_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_80, y_pred)\n",
    "\n",
    "    # Print evaluation metrics for the test data\n",
    "    print(\"Test AUC:\", auc)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac2b2b",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "919be70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 9114  1344]\n",
      " [13859 22862]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55     10458\n",
      "           1       0.94      0.62      0.75     36721\n",
      "\n",
      "    accuracy                           0.68     47179\n",
      "   macro avg       0.67      0.75      0.65     47179\n",
      "weighted avg       0.82      0.68      0.70     47179\n",
      "\n",
      "\n",
      "AUC: 0.747036237321498\n",
      "Accuracy: 0.6777591725131944\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise and train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled_70)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test_70, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_70, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "confusion_mat = confusion_matrix(y_test_70, y_pred)\n",
    "class_report = classification_report(y_test_70, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "confusion_mat = confusion_matrix(y_test_70, y_pred)\n",
    "class_report = classification_report(y_test_70, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nAUC:\", auc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf23fdd",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b879c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7451126129884722\n",
      "Average Accuracy: 0.7450981408992405\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7479896776751963\n",
      "Test Accuracy: 0.6791369041310753\n",
      "---------------------------------------------\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7452153304382247\n",
      "Average Accuracy: 0.7452053720709627\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7479896776751963\n",
      "Test Accuracy: 0.6791369041310753\n",
      "---------------------------------------------\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7451369120351143\n",
      "Average Accuracy: 0.7451288328948448\n",
      "---------------------------------------------\n",
      "Test AUC: 0.7479896776751963\n",
      "Test Accuracy: 0.6791369041310753\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# List of k values for k-fold cross-validation\n",
    "num_folds_list = [5, 10, 20]\n",
    "\n",
    "# Initialise logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "    \n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test_scaled_70)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_test_70, y_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_70, y_pred)\n",
    "\n",
    "    # Print evaluation metrics for the test data\n",
    "    print(\"Test AUC:\", auc)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760839c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64598ba2",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b1594",
   "metadata": {},
   "source": [
    "#### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35dbd0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7441381916219032\n",
      "Test AUC: 0.7481123499641426\n",
      "Training Accuracy: 0.7441381916219032\n",
      "Test Accuracy: 0.6848631291132802\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise SGD classifier model\n",
    "model = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_80)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438c1e4",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e2d979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7438552938226421\n",
      "Average Accuracy: 0.7438165139504831\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7441381916219032\n",
      "Test AUC: 0.7481123499641426\n",
      "Training Accuracy: 0.7441381916219032\n",
      "Test Accuracy: 0.6848631291132802\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7430257612809805\n",
      "Average Accuracy: 0.7430097096972139\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7441381916219032\n",
      "Test AUC: 0.7481123499641426\n",
      "Training Accuracy: 0.7441381916219032\n",
      "Test Accuracy: 0.6848631291132802\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7436270408590044\n",
      "Average Accuracy: 0.7436378667502263\n",
      "---------------------------------------------\n",
      "Training AUC: 0.7441381916219032\n",
      "Test AUC: 0.7481123499641426\n",
      "Training Accuracy: 0.7441381916219032\n",
      "Test Accuracy: 0.6848631291132802\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# List of k values for k-fold cross-validation\n",
    "num_folds_list = [5, 10, 20]\n",
    "\n",
    "# Initialise SGD classifier model\n",
    "model = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_80)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421fbfa",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2232fc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7430440465908825\n",
      "Test AUC: 0.744983151212081\n",
      "Training Accuracy: 0.7430440465908825\n",
      "Test Accuracy: 0.6829733567900973\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise SGD classifier model\n",
    "model = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_70)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4312d",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ca213ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7446670782625764\n",
      "Average Accuracy: 0.7446780083861709\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7430440465908825\n",
      "Test AUC: 0.744983151212081\n",
      "Training Accuracy: 0.7430440465908825\n",
      "Test Accuracy: 0.6829733567900973\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7442356886883358\n",
      "Average Accuracy: 0.7442228423142612\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7430440465908825\n",
      "Test AUC: 0.744983151212081\n",
      "Training Accuracy: 0.7430440465908825\n",
      "Test Accuracy: 0.6829733567900973\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7447565050471809\n",
      "Average Accuracy: 0.7447713889305675\n",
      "---------------------------------------------\n",
      "Training AUC: 0.7430440465908825\n",
      "Test AUC: 0.744983151212081\n",
      "Training Accuracy: 0.7430440465908825\n",
      "Test Accuracy: 0.6829733567900973\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# List of k values for k-fold cross-validation\n",
    "num_folds_list = [5, 10, 20]\n",
    "\n",
    "# Initialise SGD classifier model\n",
    "model = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_70)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afead1ed",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8109d8",
   "metadata": {},
   "source": [
    "#### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fed698e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.699421918125272\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7392935491050139\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_80)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2283711",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b1f8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7750413735274513\n",
      "Average Accuracy: 0.7750250278425215\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6991559432452247\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7390392013480431\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7783186258897432\n",
      "Average Accuracy: 0.7783031364351969\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6997287410098945\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7396114838012272\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.779328682916667\n",
      "Average Accuracy: 0.7793243308462295\n",
      "---------------------------------------------\n",
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6996584141276633\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7391027882872858\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_80)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e2a2d",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a93915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6940507663423837\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7337798596833337\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_70)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744e743",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a22f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7770668258652903\n",
      "Average Accuracy: 0.7770710199095345\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6946023755473094\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7345853027830178\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7800169030557255\n",
      "Average Accuracy: 0.780017975708694\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6947119195808679\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7345429110409293\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7807326886508663\n",
      "Average Accuracy: 0.7807357529894909\n",
      "---------------------------------------------\n",
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6947930021642316\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7348820449776383\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_70)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025f761",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f78509",
   "metadata": {},
   "source": [
    "#### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ccef4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8992514450276751\n",
      "Test AUC: 0.7010449906203091\n",
      "Training Accuracy: 0.8992514450276751\n",
      "Test Accuracy: 0.7490859377483865\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise Random Forest classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_80)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df862c4",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cebd6d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.775262150360527\n",
      "Average Accuracy: 0.7752445870141736\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6996688575539618\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7390392013480431\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7777650235268664\n",
      "Average Accuracy: 0.777751671843531\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6990324735308797\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7391663752265285\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7792576982301085\n",
      "Average Accuracy: 0.7792528491322667\n",
      "---------------------------------------------\n",
      "Training AUC: 0.8993076121811238\n",
      "Test AUC: 0.6993620346693397\n",
      "Training Accuracy: 0.8993076121811238\n",
      "Test Accuracy: 0.7387212666518297\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_80)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcdce96",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b705710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9037954296118205\n",
      "Test AUC: 0.6960452695171477\n",
      "Training Accuracy: 0.9037954296118205\n",
      "Test Accuracy: 0.7464125988257487\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise Random Forest classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_70)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c83119",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b1610b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7770728294052561\n",
      "Average Accuracy: 0.7770768499582211\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6945270256156021\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7346276945251065\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7798484744825107\n",
      "Average Accuracy: 0.7798487478183584\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6943975182052898\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7344793234277963\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7808829080087295\n",
      "Average Accuracy: 0.7808874783040382\n",
      "---------------------------------------------\n",
      "Training AUC: 0.9038771270511893\n",
      "Test AUC: 0.6940973475548091\n",
      "Training Accuracy: 0.9038771270511893\n",
      "Test Accuracy: 0.7342249729752645\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gregl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise decision tree classifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_70)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab78b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c49f75ad",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f019a",
   "metadata": {},
   "source": [
    "#### 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2119cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7676109556585854\n",
      "Test AUC: 0.7505888627716325\n",
      "Training Accuracy: 0.7676109556585854\n",
      "Test Accuracy: 0.7010142116809207\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise LightGBM classifier model\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_80)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bccb26",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c797223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7648755892156187\n",
      "Average Accuracy: 0.7648587599266092\n",
      "---------------------------------------------\n",
      "Training AUC: 0.7676109556585854\n",
      "Test AUC: 0.7505888627716325\n",
      "Training Accuracy: 0.7676109556585854\n",
      "Test Accuracy: 0.7010142116809207\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.7655487824404748\n",
      "Average Accuracy: 0.7655429778817322\n",
      "---------------------------------------------\n",
      "Training AUC: 0.7676109556585854\n",
      "Test AUC: 0.7505888627716325\n",
      "Training Accuracy: 0.7676109556585854\n",
      "Test Accuracy: 0.7010142116809207\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7649567222737803\n",
      "Average Accuracy: 0.7649404973598922\n",
      "---------------------------------------------\n",
      "Training AUC: 0.7676109556585854\n",
      "Test AUC: 0.7505888627716325\n",
      "Training Accuracy: 0.7676109556585854\n",
      "Test Accuracy: 0.7010142116809207\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise LightGBM classifier model\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Initialise LightGBM classifier model\n",
    "        model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_80)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_80, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_80, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf750d",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "809731d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.769549030134684\n",
      "Test AUC: 0.7512973656013996\n",
      "Training Accuracy: 0.769549030134684\n",
      "Test Accuracy: 0.7011594141461244\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise LightGBM classifier model\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = model.predict(X_test_scaled_70)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate AUC and accuracy for test and training data\n",
    "train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "# Print metrics for training and test data\n",
    "print(\"Training AUC:\", train_auc)\n",
    "print(\"Test AUC:\", test_auc)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4787b",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b1d9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Folds: 5\n",
      "Average AUC: 0.7673676751085572\n",
      "Average Accuracy: 0.7673607174466086\n",
      "---------------------------------------------\n",
      "Training AUC: 0.769549030134684\n",
      "Test AUC: 0.7512973656013996\n",
      "Training Accuracy: 0.769549030134684\n",
      "Test Accuracy: 0.7011594141461244\n",
      "=============================================\n",
      "Number of Folds: 10\n",
      "Average AUC: 0.767218307971391\n",
      "Average Accuracy: 0.7672148366650544\n",
      "---------------------------------------------\n",
      "Training AUC: 0.769549030134684\n",
      "Test AUC: 0.7512973656013996\n",
      "Training Accuracy: 0.769549030134684\n",
      "Test Accuracy: 0.7011594141461244\n",
      "=============================================\n",
      "Number of Folds: 20\n",
      "Average AUC: 0.7674007826989743\n",
      "Average Accuracy: 0.7674132602821345\n",
      "---------------------------------------------\n",
      "Training AUC: 0.769549030134684\n",
      "Test AUC: 0.7512973656013996\n",
      "Training Accuracy: 0.769549030134684\n",
      "Test Accuracy: 0.7011594141461244\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# Initialise LightGBM classifier model\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "auc_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    print(f\"Number of Folds: {num_folds}\")\n",
    "\n",
    "    # Initialise k-fold cross-validation\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store evaluation metrics for each fold\n",
    "    fold_auc_scores = []\n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, y_fold_train = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_fold_val, y_fold_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Initialise LightGBM classifier model\n",
    "        model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "        # Train the model on the fold training data\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict on the fold validation data\n",
    "        y_fold_val_pred = model.predict(X_fold_val)\n",
    "\n",
    "        # Calculate AUC and accuracy for the fold\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_fold_val_pred)\n",
    "        fold_accuracy = accuracy_score(y_fold_val, y_fold_val_pred)\n",
    "\n",
    "        fold_auc_scores.append(fold_auc)\n",
    "        fold_accuracy_scores.append(fold_accuracy)\n",
    "\n",
    "    # Calculate average AUC and accuracy for the current number of folds\n",
    "    avg_auc = sum(fold_auc_scores) / num_folds\n",
    "    avg_accuracy = sum(fold_accuracy_scores) / num_folds\n",
    "\n",
    "    # Print average metrics for cross-validation\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # Train the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_test_pred = model.predict(X_test_scaled_70)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate AUC and accuracy for test and training data\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_70, y_test_pred)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_70, y_test_pred)\n",
    "\n",
    "    # Print metrics for training and test data\n",
    "    print(\"Training AUC:\", train_auc)\n",
    "    print(\"Test AUC:\", test_auc)\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9e26d",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32119bc0",
   "metadata": {},
   "source": [
    "#### 8020 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309840",
   "metadata": {},
   "source": [
    "Loop over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dab4545",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epochs: 10\n",
      "Epoch 1/10\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5179 - accuracy: 0.7304 - val_loss: 0.7002 - val_accuracy: 0.7433\n",
      "Epoch 2/10\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5130 - accuracy: 0.7336 - val_loss: 0.7738 - val_accuracy: 0.6266\n",
      "Epoch 3/10\n",
      "4897/4897 [==============================] - 5s 985us/step - loss: 0.5112 - accuracy: 0.7347 - val_loss: 0.7345 - val_accuracy: 0.6944\n",
      "Epoch 4/10\n",
      "4897/4897 [==============================] - 5s 987us/step - loss: 0.5099 - accuracy: 0.7363 - val_loss: 0.6989 - val_accuracy: 0.7337\n",
      "Epoch 5/10\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5091 - accuracy: 0.7367 - val_loss: 0.7291 - val_accuracy: 0.6988\n",
      "Epoch 6/10\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5084 - accuracy: 0.7372 - val_loss: 0.7222 - val_accuracy: 0.7124\n",
      "Epoch 7/10\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5079 - accuracy: 0.7376 - val_loss: 0.7542 - val_accuracy: 0.6863\n",
      "Epoch 8/10\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5074 - accuracy: 0.7369 - val_loss: 0.6948 - val_accuracy: 0.7154\n",
      "Epoch 9/10\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5068 - accuracy: 0.7393 - val_loss: 0.7377 - val_accuracy: 0.6762\n",
      "Epoch 10/10\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5063 - accuracy: 0.7378 - val_loss: 0.7327 - val_accuracy: 0.6895\n",
      "Test Loss: 0.4537906050682068\n",
      "Test Accuracy: 0.7482910752296448\n",
      "=============================================\n",
      " Epochs: 20\n",
      "Epoch 1/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5190 - accuracy: 0.7293 - val_loss: 0.6988 - val_accuracy: 0.7221\n",
      "Epoch 2/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5125 - accuracy: 0.7341 - val_loss: 0.7371 - val_accuracy: 0.6864\n",
      "Epoch 3/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5110 - accuracy: 0.7341 - val_loss: 0.6921 - val_accuracy: 0.7456\n",
      "Epoch 4/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5098 - accuracy: 0.7351 - val_loss: 0.7127 - val_accuracy: 0.7188\n",
      "Epoch 5/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5088 - accuracy: 0.7362 - val_loss: 0.7388 - val_accuracy: 0.6919\n",
      "Epoch 6/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5082 - accuracy: 0.7369 - val_loss: 0.7647 - val_accuracy: 0.6566\n",
      "Epoch 7/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5075 - accuracy: 0.7374 - val_loss: 0.6880 - val_accuracy: 0.7433\n",
      "Epoch 8/20\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5071 - accuracy: 0.7383 - val_loss: 0.7420 - val_accuracy: 0.6949\n",
      "Epoch 9/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5063 - accuracy: 0.7381 - val_loss: 0.7166 - val_accuracy: 0.7004\n",
      "Epoch 10/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5058 - accuracy: 0.7387 - val_loss: 0.7280 - val_accuracy: 0.7026\n",
      "Epoch 11/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5052 - accuracy: 0.7392 - val_loss: 0.6983 - val_accuracy: 0.7311\n",
      "Epoch 12/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5047 - accuracy: 0.7396 - val_loss: 0.7283 - val_accuracy: 0.7015\n",
      "Epoch 13/20\n",
      "4897/4897 [==============================] - 7s 2ms/step - loss: 0.5043 - accuracy: 0.7399 - val_loss: 0.6799 - val_accuracy: 0.7324\n",
      "Epoch 14/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5039 - accuracy: 0.7398 - val_loss: 0.7180 - val_accuracy: 0.6855\n",
      "Epoch 15/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5035 - accuracy: 0.7404 - val_loss: 0.7246 - val_accuracy: 0.6814\n",
      "Epoch 16/20\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5033 - accuracy: 0.7403 - val_loss: 0.7332 - val_accuracy: 0.7103\n",
      "Epoch 17/20\n",
      "4897/4897 [==============================] - 8s 2ms/step - loss: 0.5028 - accuracy: 0.7405 - val_loss: 0.7060 - val_accuracy: 0.7002\n",
      "Epoch 18/20\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5027 - accuracy: 0.7407 - val_loss: 0.7129 - val_accuracy: 0.6985\n",
      "Epoch 19/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5021 - accuracy: 0.7409 - val_loss: 0.7092 - val_accuracy: 0.7174\n",
      "Epoch 20/20\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5018 - accuracy: 0.7420 - val_loss: 0.6749 - val_accuracy: 0.7517\n",
      "Test Loss: 0.46737080812454224\n",
      "Test Accuracy: 0.7324579358100891\n",
      "=============================================\n",
      " Epochs: 30\n",
      "Epoch 1/30\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5177 - accuracy: 0.7306 - val_loss: 0.6722 - val_accuracy: 0.7751\n",
      "Epoch 2/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5121 - accuracy: 0.7346 - val_loss: 0.6748 - val_accuracy: 0.7465\n",
      "Epoch 3/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5108 - accuracy: 0.7353 - val_loss: 0.7302 - val_accuracy: 0.6993\n",
      "Epoch 4/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5098 - accuracy: 0.7359 - val_loss: 0.7288 - val_accuracy: 0.7093\n",
      "Epoch 5/30\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5090 - accuracy: 0.7367 - val_loss: 0.6854 - val_accuracy: 0.7475\n",
      "Epoch 6/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5081 - accuracy: 0.7380 - val_loss: 0.7274 - val_accuracy: 0.6943\n",
      "Epoch 7/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5075 - accuracy: 0.7378 - val_loss: 0.6873 - val_accuracy: 0.7260\n",
      "Epoch 8/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5068 - accuracy: 0.7376 - val_loss: 0.7606 - val_accuracy: 0.6335\n",
      "Epoch 9/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5065 - accuracy: 0.7379 - val_loss: 0.7296 - val_accuracy: 0.6988\n",
      "Epoch 10/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5056 - accuracy: 0.7385 - val_loss: 0.7223 - val_accuracy: 0.7015\n",
      "Epoch 11/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5052 - accuracy: 0.7387 - val_loss: 0.7473 - val_accuracy: 0.6897\n",
      "Epoch 12/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5046 - accuracy: 0.7390 - val_loss: 0.7090 - val_accuracy: 0.7057\n",
      "Epoch 13/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5041 - accuracy: 0.7395 - val_loss: 0.7089 - val_accuracy: 0.6996\n",
      "Epoch 14/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5037 - accuracy: 0.7400 - val_loss: 0.6866 - val_accuracy: 0.7447\n",
      "Epoch 15/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5032 - accuracy: 0.7398 - val_loss: 0.7225 - val_accuracy: 0.6913\n",
      "Epoch 16/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5029 - accuracy: 0.7397 - val_loss: 0.7235 - val_accuracy: 0.6964\n",
      "Epoch 17/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5026 - accuracy: 0.7410 - val_loss: 0.6889 - val_accuracy: 0.7294\n",
      "Epoch 18/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5023 - accuracy: 0.7410 - val_loss: 0.7360 - val_accuracy: 0.6587\n",
      "Epoch 19/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5018 - accuracy: 0.7410 - val_loss: 0.7263 - val_accuracy: 0.7050\n",
      "Epoch 20/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5015 - accuracy: 0.7413 - val_loss: 0.7172 - val_accuracy: 0.6908\n",
      "Epoch 21/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.5012 - accuracy: 0.7420 - val_loss: 0.6786 - val_accuracy: 0.7375\n",
      "Epoch 22/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5007 - accuracy: 0.7422 - val_loss: 0.6811 - val_accuracy: 0.7320\n",
      "Epoch 23/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5006 - accuracy: 0.7422 - val_loss: 0.7127 - val_accuracy: 0.7030\n",
      "Epoch 24/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.5003 - accuracy: 0.7421 - val_loss: 0.7115 - val_accuracy: 0.7101\n",
      "Epoch 25/30\n",
      "4897/4897 [==============================] - 7s 1ms/step - loss: 0.5001 - accuracy: 0.7419 - val_loss: 0.6721 - val_accuracy: 0.7182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.4998 - accuracy: 0.7419 - val_loss: 0.7094 - val_accuracy: 0.7092\n",
      "Epoch 27/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.4996 - accuracy: 0.7420 - val_loss: 0.7262 - val_accuracy: 0.6788\n",
      "Epoch 28/30\n",
      "4897/4897 [==============================] - 6s 1ms/step - loss: 0.4993 - accuracy: 0.7431 - val_loss: 0.7040 - val_accuracy: 0.6997\n",
      "Epoch 29/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.4992 - accuracy: 0.7428 - val_loss: 0.6955 - val_accuracy: 0.6986\n",
      "Epoch 30/30\n",
      "4897/4897 [==============================] - 5s 1ms/step - loss: 0.4987 - accuracy: 0.7428 - val_loss: 0.7101 - val_accuracy: 0.6813\n",
      "Test Loss: 0.45868125557899475\n",
      "Test Accuracy: 0.7487680315971375\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_80.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_80[\"PolicyIssued\"]\n",
    "\n",
    "# List of epochs to iterate over\n",
    "epochs_list = [10, 20, 30] \n",
    "\n",
    "# Create empty lists to store results\n",
    "results = []\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    print(f\" Epochs: {epochs}\")\n",
    "\n",
    "    # Initialise a neural network model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model on the separate test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_scaled_80, y_test_80, verbose=0)\n",
    "\n",
    "    # Print metrics for test data\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797551a0",
   "metadata": {},
   "source": [
    "#### 70/30 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bb412fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epochs: 10\n",
      "Epoch 1/10\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5178 - accuracy: 0.7285 - val_loss: 0.7190 - val_accuracy: 0.6933\n",
      "Epoch 2/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5118 - accuracy: 0.7338 - val_loss: 0.7476 - val_accuracy: 0.6895\n",
      "Epoch 3/10\n",
      "4285/4285 [==============================] - 5s 1ms/step - loss: 0.5101 - accuracy: 0.7354 - val_loss: 0.7304 - val_accuracy: 0.6774\n",
      "Epoch 4/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5089 - accuracy: 0.7371 - val_loss: 0.7711 - val_accuracy: 0.6301\n",
      "Epoch 5/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5083 - accuracy: 0.7368 - val_loss: 0.7317 - val_accuracy: 0.6830\n",
      "Epoch 6/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5072 - accuracy: 0.7379 - val_loss: 0.7124 - val_accuracy: 0.6851\n",
      "Epoch 7/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5065 - accuracy: 0.7380 - val_loss: 0.6912 - val_accuracy: 0.6906\n",
      "Epoch 8/10\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5062 - accuracy: 0.7384 - val_loss: 0.7296 - val_accuracy: 0.6749\n",
      "Epoch 9/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5053 - accuracy: 0.7381 - val_loss: 0.7031 - val_accuracy: 0.7005\n",
      "Epoch 10/10\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5047 - accuracy: 0.7391 - val_loss: 0.7335 - val_accuracy: 0.6889\n",
      "Test Loss: 0.45321905612945557\n",
      "Test Accuracy: 0.7495284080505371\n",
      "=============================================\n",
      " Epochs: 20\n",
      "Epoch 1/20\n",
      "4285/4285 [==============================] - 7s 1ms/step - loss: 0.5177 - accuracy: 0.7284 - val_loss: 0.7104 - val_accuracy: 0.7410\n",
      "Epoch 2/20\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5115 - accuracy: 0.7347 - val_loss: 0.7331 - val_accuracy: 0.6669\n",
      "Epoch 3/20\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5101 - accuracy: 0.7346 - val_loss: 0.7132 - val_accuracy: 0.6902\n",
      "Epoch 4/20\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5091 - accuracy: 0.7366 - val_loss: 0.7047 - val_accuracy: 0.7035\n",
      "Epoch 5/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5083 - accuracy: 0.7371 - val_loss: 0.7170 - val_accuracy: 0.6850\n",
      "Epoch 6/20\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5075 - accuracy: 0.7370 - val_loss: 0.7065 - val_accuracy: 0.7176\n",
      "Epoch 7/20\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5070 - accuracy: 0.7384 - val_loss: 0.7417 - val_accuracy: 0.6827\n",
      "Epoch 8/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5062 - accuracy: 0.7378 - val_loss: 0.7217 - val_accuracy: 0.7118\n",
      "Epoch 9/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5054 - accuracy: 0.7384 - val_loss: 0.6789 - val_accuracy: 0.7061\n",
      "Epoch 10/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5050 - accuracy: 0.7388 - val_loss: 0.7309 - val_accuracy: 0.6976\n",
      "Epoch 11/20\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5043 - accuracy: 0.7398 - val_loss: 0.7152 - val_accuracy: 0.7013\n",
      "Epoch 12/20\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5038 - accuracy: 0.7400 - val_loss: 0.7341 - val_accuracy: 0.6963\n",
      "Epoch 13/20\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5032 - accuracy: 0.7401 - val_loss: 0.6898 - val_accuracy: 0.7201\n",
      "Epoch 14/20\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5025 - accuracy: 0.7397 - val_loss: 0.7241 - val_accuracy: 0.6738\n",
      "Epoch 15/20\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5021 - accuracy: 0.7407 - val_loss: 0.6686 - val_accuracy: 0.7207\n",
      "Epoch 16/20\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5018 - accuracy: 0.7402 - val_loss: 0.7358 - val_accuracy: 0.6914\n",
      "Epoch 17/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5014 - accuracy: 0.7404 - val_loss: 0.7242 - val_accuracy: 0.6774\n",
      "Epoch 18/20\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5009 - accuracy: 0.7410 - val_loss: 0.7162 - val_accuracy: 0.6750\n",
      "Epoch 19/20\n",
      "4285/4285 [==============================] - 6s 2ms/step - loss: 0.5005 - accuracy: 0.7411 - val_loss: 0.7061 - val_accuracy: 0.6911\n",
      "Epoch 20/20\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5000 - accuracy: 0.7416 - val_loss: 0.6976 - val_accuracy: 0.6907\n",
      "Test Loss: 0.4600769579410553\n",
      "Test Accuracy: 0.7498887181282043\n",
      "=============================================\n",
      " Epochs: 30\n",
      "Epoch 1/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5183 - accuracy: 0.7278 - val_loss: 0.6933 - val_accuracy: 0.7382\n",
      "Epoch 2/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5119 - accuracy: 0.7336 - val_loss: 0.7603 - val_accuracy: 0.6518\n",
      "Epoch 3/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5104 - accuracy: 0.7350 - val_loss: 0.6605 - val_accuracy: 0.7530\n",
      "Epoch 4/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5090 - accuracy: 0.7354 - val_loss: 0.7153 - val_accuracy: 0.6888\n",
      "Epoch 5/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5081 - accuracy: 0.7359 - val_loss: 0.7624 - val_accuracy: 0.6301\n",
      "Epoch 6/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5077 - accuracy: 0.7365 - val_loss: 0.6851 - val_accuracy: 0.7073\n",
      "Epoch 7/30\n",
      "4285/4285 [==============================] - 6s 2ms/step - loss: 0.5070 - accuracy: 0.7379 - val_loss: 0.7023 - val_accuracy: 0.6918\n",
      "Epoch 8/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5062 - accuracy: 0.7380 - val_loss: 0.6992 - val_accuracy: 0.6944\n",
      "Epoch 9/30\n",
      "4285/4285 [==============================] - 9s 2ms/step - loss: 0.5054 - accuracy: 0.7383 - val_loss: 0.7371 - val_accuracy: 0.6934\n",
      "Epoch 10/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5052 - accuracy: 0.7385 - val_loss: 0.7274 - val_accuracy: 0.6960\n",
      "Epoch 11/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.5044 - accuracy: 0.7395 - val_loss: 0.7027 - val_accuracy: 0.6946\n",
      "Epoch 12/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5039 - accuracy: 0.7394 - val_loss: 0.6972 - val_accuracy: 0.6994\n",
      "Epoch 13/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5035 - accuracy: 0.7399 - val_loss: 0.6837 - val_accuracy: 0.7254\n",
      "Epoch 14/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5028 - accuracy: 0.7402 - val_loss: 0.7099 - val_accuracy: 0.7142\n",
      "Epoch 15/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5024 - accuracy: 0.7404 - val_loss: 0.6509 - val_accuracy: 0.7444\n",
      "Epoch 16/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5020 - accuracy: 0.7403 - val_loss: 0.7104 - val_accuracy: 0.7228\n",
      "Epoch 17/30\n",
      "4285/4285 [==============================] - 6s 2ms/step - loss: 0.5015 - accuracy: 0.7413 - val_loss: 0.7162 - val_accuracy: 0.7045\n",
      "Epoch 18/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5012 - accuracy: 0.7415 - val_loss: 0.6861 - val_accuracy: 0.6971\n",
      "Epoch 19/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.5009 - accuracy: 0.7418 - val_loss: 0.7342 - val_accuracy: 0.6813\n",
      "Epoch 20/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5005 - accuracy: 0.7415 - val_loss: 0.7533 - val_accuracy: 0.6669\n",
      "Epoch 21/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.5002 - accuracy: 0.7421 - val_loss: 0.7233 - val_accuracy: 0.6840\n",
      "Epoch 22/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.4997 - accuracy: 0.7415 - val_loss: 0.7374 - val_accuracy: 0.6759\n",
      "Epoch 23/30\n",
      "4285/4285 [==============================] - 6s 1ms/step - loss: 0.4998 - accuracy: 0.7420 - val_loss: 0.7272 - val_accuracy: 0.6837\n",
      "Epoch 24/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.4990 - accuracy: 0.7428 - val_loss: 0.7158 - val_accuracy: 0.7013\n",
      "Epoch 25/30\n",
      "4285/4285 [==============================] - 8s 2ms/step - loss: 0.4989 - accuracy: 0.7418 - val_loss: 0.7029 - val_accuracy: 0.7096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.4984 - accuracy: 0.7431 - val_loss: 0.6921 - val_accuracy: 0.7093\n",
      "Epoch 27/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.4981 - accuracy: 0.7428 - val_loss: 0.6993 - val_accuracy: 0.6867\n",
      "Epoch 28/30\n",
      "4285/4285 [==============================] - 7s 2ms/step - loss: 0.4978 - accuracy: 0.7438 - val_loss: 0.6330 - val_accuracy: 0.7636\n",
      "Epoch 29/30\n",
      "4285/4285 [==============================] - 6s 2ms/step - loss: 0.4975 - accuracy: 0.7440 - val_loss: 0.6973 - val_accuracy: 0.6823\n",
      "Epoch 30/30\n",
      "4285/4285 [==============================] - 6s 2ms/step - loss: 0.4973 - accuracy: 0.7440 - val_loss: 0.7260 - val_accuracy: 0.6634\n",
      "Test Loss: 0.4540114998817444\n",
      "Test Accuracy: 0.7569893598556519\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X_train = resampled_df_70.drop(\"PolicyIssued\", axis=1)\n",
    "y_train = resampled_df_70[\"PolicyIssued\"]\n",
    "\n",
    "# List of epochs to iterate over\n",
    "epochs_list = [10, 20, 30] \n",
    "\n",
    "# Create empty lists to store results\n",
    "results = []\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    print(f\" Epochs: {epochs}\")\n",
    "\n",
    "    # Initialise a neural network model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model on the separate test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_scaled_70, y_test_70, verbose=0)\n",
    "\n",
    "    # Print metrics for test data\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"=============================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
